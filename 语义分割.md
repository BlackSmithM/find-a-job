# 语义分割

https://api.bbs.cvmart.net/articles/4820

## 网络模型：

### OCR：

https://blog.csdn.net/moxibingdao/article/details/108162695

基于物体区域的上下文信息

微软亚洲研究院提出的 OCR 方法的主要思想是显式地把像素分类问题转化成物体区域分类问题，这与语义分割问题的原始定义是一致的，即每一个像素的类别就是该像素属于的物体的类别，换言之，与 PSPNet 和 DeepLabv3 的上下文信息最主要的不同就在于 OCR 方法显式地增强了物体信息。

OCR 方法的实现主要包括3个阶段：

（1）根据网络中间层的特征表示估测一个**粗略的语义分割结果**作为 OCR 方法的一个输入 ，即软物体区域（Soft Object Regions），

（2）根据**粗略的语义分割结果**和**网络最深层的特征表示**计算出 K 组向量，即物体区域表示（Object Region Representations），其中**每一个向量对应一个语义类别的特征表示**，

（3）计算网络最深层输出的像素特征表示（Pixel Representations）与计算得到的物体区域特征表示（Object Region Representation）之间的关系矩阵，然后根据每个像素和物体区域特征表示在关系矩阵中的数值把物体区域特征加权求和，得到最后的物体上下文特征表示 OCR (Object Contextual Representation) 。

当把物体上下文特征表示 OCR 与网络最深层输入的特征表示拼接之后作为上下文信息增强的特征表示（Augmented Representation），可以基于增强后的特征表示预测每个像素的语义类别，具体算法框架可以参考图6。

综上，OCR 可计算一组物体区域的特征表达，然后根据物体区域特征表示与像素特征表示之间的相似度将这些物体区域特征表示传播给每一个像素。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Ia1B2d0N1RndOT3h3T2pDSGdRMWVGb0pKcGRaNWlhREpnSURDNGw4YkJxTmowOGQ1NzlYWGhPaWNPdnBOYnZLWFI4U0M3NURkTHByWWJmd0NlNHRqdmRBLzY0MA?x-oss-process=image/format,png)

图6：OCRNet 框架

图7中对比了基于 ASPP 的多尺度上下文信息与基于 OCR 的物体上下文信息的区别。对选定的红色标记的像素，我们用蓝色来标记其对应的上下文信息。可以看到基于 ASPP 的多尺度上下文信息通常会包含不属于红色像素所属类别的像素，左图中采样到的蓝色标记的像素有一部分落在了人的身体上，还有一部分像素落在了后面的展板上。

因此，这样的多尺度信息同时包含了物体信息与背景信息。而基于 OCR 的物体上下文信息的目标是只利用物体信息，即显式地增强物体信息。 

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Ia1B2d0N1RndOT3h3T2pDSGdRMWVGb0pKcGRaNWlhREp4YXlTbEl2MW55SUhmMkZFRHN2TnRUWFNyMHdIeG9vd25uM2JtZE10d0g0aGZwNHl1V2c1cUEvNjQw?x-oss-process=image/format,png)

图7：ASPP 与 OCR 方法的对比

### HRNet：

1、引入

![img](https://pic4.zhimg.com/80/v2-615368a89e9a372b19391d14e353e23f_720w.jpg)

网络结构设计思路

在人体姿态识别这类的任务中，需要生成一个高分辨率的heatmap来进行关键点检测。这就与一般的网络结构比如VGGNet的要求不同，因为VGGNet最终得到的feature map分辨率很低，损失了空间结构。

![img](https://pic4.zhimg.com/80/v2-eab275cc5013b5a276b5334819094f83_720w.png)

传统的解决思路

获取高分辨率的方式大部分都是如上图所示，采用的是先降分辨率，然后再升分辨率的方法。U-Net、SegNet、DeconvNet、Hourglass本质上都是这种结构。

![img](https://pic4.zhimg.com/80/v2-90155835087643727995f10543130e8b_720w.jpg)

2、核心

普通网络都是这种结构，，不同分辨率之间是进行了串联

![img](https://pic1.zhimg.com/80/v2-f5b103d036fb3f7f856085d3cccd5c24_720w.jpg)

HRNet是将不同分辨率的feature map进行并联：

![img](https://pic2.zhimg.com/80/v2-4416219bdf5268dfdae6c3a02265f33d_720w.jpg)

在并联的基础上，添加不同分辨率feature map之间的交互(fusion)。

![img](https://pic2.zhimg.com/80/v2-c554925bd80e8ca3b10d53ba79ff8ba9_720w.jpg)

具体fusion的方法如下图所示：

![img](https://pic4.zhimg.com/80/v2-d99d67e621f868b5b3e15b5cae8e751b_720w.jpg)

- 同分辨率的层直接复制。
- 需要升分辨率的使用bilinear upsample + 1x1卷积将channel数统一。
- 需要降分辨率的使用strided 3x3 卷积。
- 三个feature map融合的方式是相加。

至于为何要用strided 3x3卷积，这是因为卷积在降维的时候会出现信息损失，使用strided 3x3卷积是为了通过学习的方式，降低信息的损耗。所以这里没有用maxpool或者组合池化。

![img](https://pic3.zhimg.com/80/v2-64f63e1f6132f958f237e4f345596f1e_720w.jpg)

另外在读HRNet的时候会有一个问题，有四个分支的到底如何使用这几个分支呢？论文中也给出了几种方式作为最终的特征选择。

![img](https://pic3.zhimg.com/80/v2-ec32909699ec46c1155dad6022438dfe_720w.jpg)

(a)图展示的是HRNetV1的特征选择，只使用分辨率最高的特征图。

(b)图展示的是HRNetV2的特征选择，将所有分辨率的特征图(小的特征图进行upsample)进行concate，主要用于语义分割和面部关键点检测。

(c)图展示的是HRNetV2p的特征选择，在HRNetV2的基础上，使用了一个特征金字塔，主要用于目标检测网络。再补充一个(d)图

![img](https://pic3.zhimg.com/80/v2-00d6fa1b7b541b94be1a1972c639478a_720w.jpg)

(d)图展示的也是HRNetV2，采用上图的融合方式，主要用于训练分类网络。总结一下HRNet**创新点**：

- 将高低分辨率之间的链接由串联改为并联。
- 在整个网络结构中都保持了高分辨率的表征(最上边那个通路)。
- 在高低分辨率中引入了交互来提高模型性能。



### U-Net：

#### 简要阐述一下UNet网络

UNet网络可以简单看为先下采样，经过不同程度的卷积，学习了深层次的特征，再经过上采样回复为原图大小，上采样用反卷积实现。输出类别数量的特征图，最后使用激活函数softmax将特征图转换为概率图，针对某个像素点，如输出是[0.1，0.9]，则判定这个像素点是第二类的概率更大。

网络结构可以看成3个部分：

①下采样：网络的红色箭头部分，池化实现

②上采样：网络的绿色箭头部分，反卷积实现

③最后层的softmax：在网络结构中，最后输出两张feature maps后，其实在最后还要做一次softmax，将其转换为概率图。

#### 简述encode和decode思想

将一个input信息编码到一个压缩空间中 将一个压缩空间向量解码到一个原始空间中。





### FCN：

#### FCN与CNN最大的区别？

卷积层不再与FC层相连,而是加入一个全局池化层。

#### FCN是如何取代FC层的

 FCN用卷积层代替FC层。





### Deeplab：

https://mp.weixin.qq.com/s/JV-R41r3S8UiiiAoZ4pCqg

#### 1.简述Deeplab v1网络

DeepLab是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。在实验中发现DCNNs做语义分割时精准度不够的问题，根本原因是DCNNs的高级特征的平移不变性（即高层次特征映射，根源在于重复的池化和下采样）。针对信号下采样或池化降低分辨率，DeepLab是采用的atrous（带孔）算法扩展感受野，获取更多的上下文信息。另外，DeepLab 采用完全连接的条件随机场（CRF）提高模型捕获细节的能力。论文模型基于 VGG16，在 Titan GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在 PASCAL VOC-2012 达到 71.6% IOU accuracy。

#### 2.简述Deeplab v2网络

DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：特征分辨率的降低、物体存在多尺度，DCNN 的平移不变性。因 DCNN 连续池化和下采样造成分辨率降低，DeepLabv2 在最后几个最大池化层中去除下采样，取而代之的是使用空洞卷积，以更高的采样密度计算特征映射。物体存在多尺度的问题，DeepLabv1 中是用多个 MLP 结合多尺度特征解决，虽然可以提供系统的性能，但是增加特征计算量和存储空间。论文受到 Spatial Pyramid Pooling (SPP) 的启发，提出了一个类似的结构，在给定的输入上以**不同采样率的空洞卷积**并行采样，相当于以多个比例捕捉图像的上下文，称为 ASPP (atrous spatial pyramid pooling) 模块。

![](https://i.loli.net/2021/07/14/Nrgu52a7pGFAbBw.jpg)

相比于DeepLab v1，deeplab v2在之前的基础上做了三个方面的贡献：一是使用Atrous Convolution 代替原来上采样的方法，比之前得到更高像素的score map，并且增加了感受野的大小；二是使用ASPP 代替原来对图像做预处理resize 的方法，使得输入图片可以具有任意尺度，而不影响神经网络中全连接层的输入大小；三是使用全连接的CRF，利用低层的细节信息对分类的局部特征进行优化。

论文模型基于 ResNet，在 NVidia Titan X GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在耗时方面和 DeepLabv1 无差异，但在 PASCAL VOC-2012 达到 79.7 mIOU。

#### 3.简述Deeplab v3网络相比于之前的v1和v2网络有哪些改进

①重新讨论了空洞卷积的使用，这让我们在级联模块和空间金字塔池化的框架下，能够获取更大的感受野从而获取多尺度信息。②改进了ASPP模块：由不同采样率的空洞卷积和BN层组成，我们尝试以级联或并行的方式布局模块。③讨论了一个重要问题：使用大采样率的3×3的空洞卷积，因为图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 我们建议将图像级特征融合到ASPP模块中。④阐述了训练细节并分享了训练经验。

##### 介绍deeplabv3,画出backbone

DeepLab V3将空洞卷积应用在了级联模块，并且改进了ASPP模块。backbone还是resnet 101. 增强ASPP模块，复制resnet最后的block级联起来，加入BN。没有使用CRFs新的ASPP模块包括：一个1×1卷积和3个3×3的空洞卷积(采样率为(6,12,18))，每个卷积核都有256个且都有BN层；包含图像级特征image-level features(即全局平均池化Global Avearge Pooling)；所有分支得到的结果concate起来通过1×1卷积之后得到最终结果。

DeepLab V3采用atrous convolution的上采样滤波器提取稠密特征映射和去捕获大范围的上下文信息。具体来说，编码多尺度信息，提出的级联模块逐步翻倍的atrous rates，提出的atrous spatial pyramid pooling模块增强图像级的特征，探讨了多采样率和有效视场下的滤波器特性。实验结果表明，该模型在Pascalvoc 2012语义图像分割基准上比以前的DeppLab版本有了明显的改进，并取得了与其他先进模型相当的性能。

##### DeepLab V3的改进

主要包括以下几方面：1）提出了更通用的框架，适用于任何网络

2）复制了ResNet最后的block，并级联起来3）在ASPP中使用BN层4）去掉了CRF。

![](https://i.loli.net/2021/07/14/xPyiMRSEQBaIXDj.jpg)

**deeplabv3的损失函数**交叉熵损失函数

#### 4.deeplabv3+系列

![](https://i.loli.net/2021/07/14/VJbSRajkzGmDFOn.jpg)

DeepLabv3 +采用编码器解码器结构扩展了DeepLabv3。编码器模块通过在多个尺度上应用atrous卷积来编码多尺度上下文信息，而简单但有效的解码器模块沿着对象边界细化分割结果。DeepLabv3+模型中使用ResNet-101作为网络主干。探索Xecption模型, 将depthwise separable convolution应用到ASPP和解码模块上(更快,更稳定)。

#### 5.条件随机场(CRF)后处理的目的

CRF使像素级别的类别标签的多类别输出与底层图像信息（如像素间的相互关系）有关，这种结合尤其重要，这也是关注于局部细节的CNN所未能考虑到的。CRF 将图像中每个像素点所属的类别都看作一个变量xi，然后考虑任意两个变量之间的关系，建立一个完全图。



### **PSPNet** 

CVPR 2017 的 PSPNet [11] 提出了用 Pyramid Pooling 模块来抽取多尺度的上下文信息，以解决物体多尺度的问题。

受益于这种更丰富的上下文信息，PSPNet 取得了 ImageNet Scene Parsing Challenge 2016 第一名的成绩。具体来说，PSPNet 采用了4路并行的不同尺度的图像划分，分别将图像均匀的划分成6ⅹ6/3ⅹ3/2ⅹ2个子区域，然后在每个子区域上应用 Average Pooling 计算得到一个向量作为这个区域内所有像素的 (不同尺度划分下) 局部上下文信息。

另外，PSPNet 也会采用 Global Pooling 计算得到的一个向量作为所有像素的全局上下文信息，PSPNet 的整体计算框架见图4。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Ia1B2d0N1RndOT3h3T2pDSGdRMWVGb0pKcGRaNWlhREpoQzNnaEFiTXN2TUhRbkxWYnVuSFFPVjVWMHN4MWJiSEkwdGlhcmMxbldZTkx0RThjd252SUZnLzY0MA?x-oss-process=image/format,png)

图4：PSPNet 框架



## 损失函数

### **①Log loss**

对于二分类而言，对数损失函数如下公式所示：

![[公式]](https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28y_%7Bi%7D+%5Clog+p_%7Bi%7D%2B%5Cleft%281-y_%7Bi%7D%5Cright%29+%5Clog+%5Cleft%281-p_%7Bi%7D%5Cright%29%5Cright%29+%5C%5C)

其中，y_i为输入实例x_i的真实类别, p_i为预测输入实例 x_i属于类别 1的概率.对所有样本的对数损失表示对每个样本的对数损失的平均值,对于完美的分类器, 对数损失为 0。

此loss function每一次梯度的回传对每一个类别具有相同的关注度！所以极易受到类别不平衡的影响。

### **②WCE Loss**

带权重的交叉熵loss — Weighted cross-entropy (WCE)

R为标准的分割图，其中r_n为label 分割图中的某一个像素的GT。P为预测的概率图，p_n为像素的预测概率值，背景像素图的概率值就为1-P。

只有两个类别的带权重的交叉熵为：

![[公式]](https://www.zhihu.com/equation?tex=W+C+E%3D-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bn%3D1%7D%5E%7BN%7D+w+r_%7Bn%7D+%5Clog+%5Cleft%28p_%7Bn%7D%5Cright%29%2B%5Cleft%281-r_%7Bn%7D%5Cright%29+%5Clog+%5Cleft%281-p_%7Bn%7D%5Cright%29+%5C%5C)

w为权重，![[公式]](https://www.zhihu.com/equation?tex=w%3D%5Cfrac%7BN-%5Csum_%7Bn%7D+p_%7Bn%7D%7D%7B%5Csum_%7Bn%7D+p_%7Bn%7D%7D)缺点是需要人为的调整困难样本的权重，增加调参难度。

### **③Focal loss**

能否使网络主动学习困难样本呢？focal loss的提出是在目标检测领域，为了解决正负样本比例严重失衡的问题。是由log loss改进而来的，为了与log loss进行对比，公式如下：

![[公式]](https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28%5Calpha+y_%7Bi%7D%5Cleft%281-p_%7Bi%7D%5Cright%29%5E%7B%5Cgamma%7D+%5Clog+p_%7Bi%7D%2B%281-%5Calpha%29%5Cleft%281-y_%7Bi%7D%5Cright%29+p_%7Bi%7D%5E%7BY%7D+%5Clog+%5Cleft%281-p_%7Bi%7D%5Cright%29%5Cright%29+%5C%5C)

 说白了就多了一个![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%281-p_%7Bi%7D%5Cright%29%5E%7B%5Cgamma%7D) ，loss随样本概率的大小如下图所示：

<img src="https://pic3.zhimg.com/80/v2-c228f0140283bde10634bd14e19d11da_720w.jpg" alt="img" style="zoom:150%;" />

其基本思想就是，对于类别极度不均衡的情况下，网络如果在log loss下会倾向于只预测负样本，并且负样本的预测概率p_i也会非常的高，回传的梯度也很大。但是如果添加![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%281-p_%7Bi%7D%5Cright%29%5E%7By%7D)则会使预测概率大的样本得到的loss变小，而预测概率小的样本，loss变得大，从而加强对正样本的关注度。

可以改善目标不均衡的现象，对此情况比 binary_crossentropy 要好很多。

目前在图像分割上只是适应于二分类。需要添加额外的两个全局参数alpha和gamma，对于调参不方便。

### **④Dice loss**

dice loss的提出是在Ｖ-net中，其中的一段原因描述是在感兴趣的解剖结构仅占据扫描的非常小的区域，从而使学习过程陷入损失函数的局部最小值。所以要加大前景区域的权重。

Dice 可以理解为是两个轮廓区域的相似程度，用A、B表示两个轮廓区域所包含的点集，定义为：

![[公式]](https://www.zhihu.com/equation?tex=D+S+C%28A%2C+B%29%3D2+%5Cfrac%7B%7CA+%5Ccap+B%7C%7D%7B%7CA%7C%2B%7CB%7C%7D+%5C%5C)

其次Dice也可以表示为：![[公式]](https://www.zhihu.com/equation?tex=D+S+C%3D%5Cfrac%7B2+T+P%7D%7B2+T+P%2BF+N%2BF+P%7D)其中TP，FP，FN分别是真阳性、假阳性、假阴性的个数。

二分类dice loss:

![[公式]](https://www.zhihu.com/equation?tex=D+L_{2}%3D1-\frac{\sum_{n%3D1}^{N}+p_{n}+r_{n}%2B\epsilon}{\sum_{n%3D1}^{N}+p_{n}%2Br_{n}%2B\boldsymbol{\epsilon}}-\frac{\sum_{n%3D1}^{N}\left(1-p_{n}\right)\left(1-r_{n}\right)%2B\boldsymbol{\epsilon}}{\sum_{n%3D1}^{N}+2-p_{n}-r_{n}%2B\boldsymbol{\epsilon}}+\\)

结论：

1.有时使用dice loss会使训练曲线有时不可信，而且dice loss好的模型并不一定在其他的评价标准上效果更好，例如mean surface distance 或者是Hausdorff surface distance。不可信的原因是梯度，对于softmax或者是log loss其梯度简化而言为p-t，t为目标值，p为预测值。而dice loss为![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B2+t%5E%7B2%7D%7D%7B%28p%2Bt%29%5E%7B2%7D%7D)，如果p，t过小则会导致梯度变化剧烈，导致训练困难。

2.属于直接在评价标准上进行优化。

3.不均衡的场景下的确好使。

### **⑤IOU loss**

可类比DICE LOSS，也是直接针对评价标准进行优化。

定义如下： ![[公式]](https://www.zhihu.com/equation?tex=+I+O+U%3D%5Cfrac%7B%7CA+%5Ccap+B%7C%7D%7B%7CA%7C%2B%7CB%7C-%7CA+%5Ccap+B%7C%7D+)

在图像分割领域评价标准IOU实际上![[公式]](https://www.zhihu.com/equation?tex=I+O+U%3D%5Cfrac%7BT+P%7D%7BT+P%2BF+P%2BF+N%7D) ，而TP，FP，FN分别是真阳性、假阳性、假阴性的个数。

而作为loss function，定义 ![[公式]](https://www.zhihu.com/equation?tex=I+O+U%3D%5Cfrac%7BI%28X%29%7D%7BU%28X%29%7D)其中，

![[公式]](https://www.zhihu.com/equation?tex=I%28X%29%3DX+%2A+Y+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=U%28X%29%3DX%2BY-X+%2A+Y+%5C%5C)

X为预测值而Y为真实标签。

 IOU loss的缺点同DICE loss是相类似的，训练曲线可能并不可信，训练的过程也可能并不稳定，有时不如使用softmax loss等的曲线有直观性，通常而言softmax loss得到的loss下降曲线较为平滑。

### **⑤mIOU**

mIoU值是一个衡量图像分割精度的重要指标。mIoU可解释为平均交并比，即在每个类别上计算IoU值（即真正样本数量/（真正样本数量+假负样本数量+假正样本数量））。

![img](https://i.loli.net/2021/07/14/cEJW3DlBdpbvTOj.jpg)

### 补充：

https://www.aiuai.cn/aifarm1330.html

https://blog.csdn.net/xijuezhu8128/article/details/111164936

### 交叉熵（Cross Entorpy）

### 加权交叉熵损失函数

### Tversky loss

### Lovasz-Softmax Loss





## 其他知识点

### 分割出来的结果通常会有不连续的情况，怎么处理？开运算闭运算

设定阈值，去掉阈值较小的连通集，和较小的空洞。

开运算 = 先腐蚀运算，再膨胀运算（看上去把细微连在一起的两块目标分开了）

**开运算**总结：

（１）开运算能够除去孤立的小点，毛刺和小桥，而总的位置和形状不便。

（２）开运算是一个基于几何运算的滤波器。

（３）结构元素大小的不同将导致滤波效果的不同。

（４）不同的结构元素的选择导致了不同的分割，即提取出不同的特征。



闭运算 = 先膨胀运算，再腐蚀运算（看上去将两个细微连接的图块封闭在一起）

**闭运算**总结：

（1）闭运算能够填平小湖（即小孔），弥合小裂缝，而总的位置和形状不变。

（2）闭运算是通过填充图像的凹角来滤波图像的。（3）结构元素大小的不同将导致滤波效果的不同。（4）不同结构元素的选择导致了不同的分割。



### 空洞卷积的具体实现

Dilated convolution就是为了在不是用pooling操作损失信息也能增加感受野。空洞卷积：在3*3卷积核中间填充0，有两种实现方式，第一，卷积核填充0，第二，输入等间隔采样。空洞卷积的rate，代表传统卷积核的相邻之间插入rate-1个空洞数。当rate=1时，相当于传统的卷积核。扩张卷积具有更大的感受野。

Pytorch实现过程如下：

![img](https://i.loli.net/2021/07/14/1eiZMFlEsz9YKIb.jpg)

